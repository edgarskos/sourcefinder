#!/bin/bash

# step 0, initialise

#mkdir -p working
#rm working/*

curl -H "Accept: text/tab-separated-values" "https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=PREFIX%20schema%3A%20%3Chttp%3A%2F%2Fschema.org%2F%3E%0APREFIX%20wikibase%3A%20%3Chttp%3A%2F%2Fwikiba.se%2Fontology%23%3E%0APREFIX%20wd%3A%20%3Chttp%3A%2F%2Fwww.wikidata.org%2Fentity%2F%3E%0APREFIX%20wdt%3A%20%3Chttp%3A%2F%2Fwww.wikidata.org%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fcid%20%3Fodnb%20%3Farticle%20WHERE%20%7B%0A%20%20%20%20%3Fcid%20wdt%3AP1415%20%3Fodnb%20.%0A%20%20%20%20OPTIONAL%20%7B%0A%20%20%20%20%20%20%3Farticle%20schema%3Aabout%20%3Fcid%20.%0A%20%20%20%20%20%20%3Farticle%20schema%3AinLanguage%20%22en%22%20.%0A%20%20%20%20%20%20%3Farticle%20schema%3AisPartOf%20%3Chttps%3A%2F%2Fen.wikipedia.org%2F%3E%20.%0A%20%20%20%20%7D%0A%7D%20" > working/sparqldownload

tr -d '\r' < working/sparqldownload > working/odnb-sparql # to get rid of blasted ^M!

# this file (which takes a couple of secs to generate) is a SPARQL query giving:
# COL 1 - Wikidata item URL
# COL 2 - ODNB number
# COL 3 - enwiki URL if this exists

grep "wikipedia.org" working/odnb-sparql | sed 's/<https\:\/\/en.wikipedia.org\/wiki\///' | sed 's/>//' | sed 's/<http\:\/\/www.wikidata.org\/entity\///' > working/odnb-trimmed

# trimmed.tsv now only has WP-matching lines - there's still 42000 of them! - and nicely tidied up

grep "wikipedia.org" working/odnb-sparql | sed 's/<https:\/\/en.wikipedia.org\/wiki\///g' | sed 's/>//g' | sed 's/%20/_/g' | sed 's/%2C/,/g' | cut -f 2-3 | sed 's/^101[0]*/id=/g' > working/odnb-trimmed

#  | tail -10
# for the purposes of testing replace it with a ten-line version?

cut -f 2 working/odnb-trimmed > working/odnb-namelist

# get the TSV table of WP articles with ODNB cites from quarry- https://quarry.wmflabs.org/query/2337 for all normal doi's

curl "https://quarry.wmflabs.org/run/91354/output/0/tsv?download=true" | uniq | sed 's/\/\/dx.doi.org\/10.1093%2Fref:odnb%2F/id=/g' | cut -f 2-3 > working/doilist

cut -f 2 working/doilist | sort | uniq > working/doi-names

# this is now id=xxxx, tab, pagename

# so now we have
# working/odnb-trimmed - sample of "all articles matching a Wikidata ODNB ID"
# working/odnb-namelist - just the page names from above
# citations.tsv - the prettified list of ODNB citations 
# working/doilist - all articles with extant DOI links that reflect an ODNB template
# working/doi-names - just the page names from above

# so! we want to make a list of all articles in the WD list, and for each one, does it have a nice citation?

# this is the absolute crudest first pass - all the articles which have no citeODNB or similar, whether it's relevant or not

grep -v -f working/doi-names working/odnb-namelist > working/articles-no-odnb

for i in `cat working/articles-no-odnb` ; 
do echo $i ;
grep $i working/odnb-trimmed | sed 's/^101[0]*/id=/g' | sed -e 's/^M//' >> working/odnb-candidates ;
done

# now we have working/odnb-candidates - ID and pagename for everything which definitely doesn't have the template

for i in `cat working/odnb-candidates | cut -f 1` ; do grep "$i|" citations.tsv >> working/candidates-cited || printf "\n" >> working/candidates-cited ;  done

paste working/odnb-candidates working/candidates-cited | cut -f 2,4 > output

rm table

# insert suitable headers here

echo -e "{|" >> table
echo -e "|-" >> table
cat output | sed 's/^/| [[/g' | sed 's/\t{/]]\t|| {/g' | sed 's/$/\n|-/g' >> table
echo -e "|}" >> table

# and then run pywikibot to upload 'table' to the appropriate location
